{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b53296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt4\n",
    "%matplotlib notebook \n",
    "\n",
    "\n",
    "\n",
    "class LinearRegressionModule():\n",
    "    def __init__(self, iteration_count, learning_alpha):\n",
    "        self.num_iterations=iteration_count\n",
    "        self.learning_rate=learning_alpha\n",
    "    \n",
    "############################# Core Functionalities   ###############################\n",
    "\n",
    "################################## Direct Methods ###################################\n",
    "        \n",
    "    def direct_method_overdetermined(self,X,Y):\n",
    "        W=(np.linalg.inv((X.T).dot(X)).dot(X.T)).dot(Y)\n",
    "        return W\n",
    "    def direct_method_underdetermined(self,X,Y):\n",
    "        W=((X.T).dot(np.linalg.inv((X).dot(X.T)))).dot(Y)\n",
    "        return W\n",
    "    \n",
    "################################## Iterative Methods functionalites ###################################   \n",
    "    \n",
    "    def grad_descent(self,X,Y,W):\n",
    "        N=X.shape[0]\n",
    "        n=X.shape[1]           \n",
    "        W=W-((X.T).dot(X.dot(W)-Y))*(self.learning_rate/N)            ## Batch gradient Descent      \n",
    "        return W  \n",
    "    \n",
    "    \n",
    "    def predict_Y(self,X,W):\n",
    "        return X.dot(W)\n",
    "    \n",
    "   \n",
    "    def stochastic_descent(self,X,Y,W,alpha):\n",
    "        N=X.shape[0]\n",
    "        #for j in range (self.num_iterations):\n",
    "        for i in range(N):\n",
    "            W=W-(alpha/N)*((X[[i],:]).T).dot(((X[[i],:]).dot(W)-Y[i]))                \n",
    "        return W\n",
    "    \n",
    "    def cost_function(self,X,Y,W):\n",
    "        N=X.shape[0]\n",
    "        n=X.shape[1]\n",
    "        j=0\n",
    "        j=(((X.dot(W)-Y).T).dot((X.dot(W)-Y)))*(1/(2*N))        \n",
    "        return j\n",
    "    \n",
    "   ################################## UtilitY functionalites ###################################  \n",
    "    \n",
    "    def write_weights_to_file(self,file_name, W):\n",
    "        param_file=open(file_name,\"w\")\n",
    "        for i in range(0,W.shape[0]):\n",
    "            param_file.write(str(W[i]))\n",
    "            param_file.write(\"\\n\")\n",
    "        param_file.close()\n",
    "    def zscore_normalization(self,X):\n",
    "        N=X.shape[0]\n",
    "        n=X.shape[1]        \n",
    "        print(\"N,n=\",N,n)        \n",
    "        data_norm=X        \n",
    "        mean=[]\n",
    "        sigma=[]        \n",
    "        mean=np.mean(data_norm,axis=0)\n",
    "        sigma=np.std(data_norm,axis=0)        \n",
    "        for j in range(data_norm.shape[1]-1):\n",
    "            for i in range(data_norm.shape[0]):\n",
    "                data_norm[i][j] = (data_norm[i][j]-mean[j])/(sigma[j])\n",
    "        return data_norm\n",
    "    \n",
    "    def minmax_normalization(self,X):\n",
    "        N=X.shape[0]\n",
    "        n=X.shape[1]        \n",
    "        print(\"N,n=\",N,n)        \n",
    "        data_norm=X\n",
    "        for j in range(data_norm.shape[1]):\n",
    "            min = np.min(data_norm[:,j])\n",
    "            max = np.max(data_norm[:,j])\n",
    "            data_norm[:,j] = (data_norm[:,j] - np.min(data_norm[:,j])) / (np.max(data_norm[:,j]) - np.min(data_norm[:,j]))\n",
    "            \n",
    "        return data_norm\n",
    "                \n",
    "    def hold_out_validation(self,X,Y,alpha_min,alpha_max,alpha_size):\n",
    "        N=X.shape[0]\n",
    "        n=X.shape[1]\n",
    "        mean_square_error = []\n",
    "        std_deviation     = []\n",
    "        alpha_arrayay       = []\n",
    "        \n",
    " \n",
    "        for alpha in np.linspace(alpha_min,alpha_max,alpha_size):            \n",
    "            error_sum=0.0\n",
    "            for i in range(50):\n",
    "                #error_sum=0.0\n",
    "                data= np.random.permutation(X)\n",
    "                zero_column_train=tuple(np.ones(((int)(0.7*N),1)))\n",
    "                zero_column_test=tuple(np.ones(((int)(0.3*N)+1,1)))\n",
    "                \n",
    "                X_train=data[0:int(0.7*N),0:n-1]\n",
    "                Y_train=data[0:int(0.7*N),n-1:n]\n",
    "                X_test=data[int(0.7*N):N,0:n-1]                \n",
    "                Y_test=data[int(0.7*N):N,n-1:n]               \n",
    "                X_train=np.hstack((zero_column_train,X_train))\n",
    "                X_test=np.hstack((zero_column_test,X_test))               \n",
    "                norm=1000000*1000000 # a very high number\n",
    "                W_old=np.zeros((n,1))\n",
    "                W_new=W_old\n",
    "                self.learning_rate=alpha\n",
    "                no_of_iterations=0\n",
    "                while(norm > 0.001 and no_of_iterations < 20000):\n",
    "                    W_new=self.grad_descent(X_train,Y_train,W_old)\n",
    "                    norm=np.linalg.norm(W_new-W_old)\n",
    "                    W_old=W_new\n",
    "                    no_of_iterations+=1\n",
    "                cost         = self.cost_function(X_test,Y_test,W_new)\n",
    "                error_sum    = error_sum + cost\n",
    "                Y_predict    = self.predict_Y(X_test,W_new)\n",
    "                diff_vec     = Y_test - Y_predict\n",
    "                std_dev      = np.std(diff_vec)\n",
    "                \n",
    "            std_deviation.append(std_dev/50)    \n",
    "            mean_square_error.append(error_sum/50)\n",
    "            alpha_arrayay.append(alpha)        \n",
    "        min_index     = mean_square_error.index(min(mean_square_error))\n",
    "        min_std_min   = min(std_deviation) \n",
    "        return min(mean_square_error),alpha_arrayay[min_index],min_std_min\n",
    "            \n",
    "       \n",
    "    def k_fold_cross_validation(self,dataset,alpha_min,alpha_max,step_size):\n",
    "        \n",
    "        N=dataset.shape[0]\n",
    "        n=dataset.shape[1]\n",
    "        print(N,n)\n",
    "        erroe_sum=0.0\n",
    "        for i in range(0,30):       \n",
    "            data= np.random.permutation(dataset)\n",
    "            X_train=data[0:int(0.7*N),0:n-1]\n",
    "            Y_train=data[0:int(0.7*N),n-1:n]\n",
    "            X_test=data[int(0.7*N):N,0:n-1]\n",
    "            Y_test=data[int(0.7*N):N,n-1:n]\n",
    "\n",
    "            alpha_kfold=self.k_fold_validation(X_train,Y_train,alpha_min,alpha_max,step_size)\n",
    "            print(\" Alpha minimum for K-Fold for iteration no\",i+1,\" is \",alpha_kfold)\n",
    "            W_old=np.zeros((n-1,1))\n",
    "            W_new=W_old\n",
    "            \n",
    "            norm=100000*100000\n",
    "            self.learning_rate=alpha_kfold\n",
    "            no_of_iterations=0\n",
    "            while(norm > 0.001 and no_of_iterations < 1000):\n",
    "                W_new=self.grad_descent(X_train,Y_train,W_old)\n",
    "                norm=np.linalg.norm(W_new-W_old)\n",
    "                W_old=W_new\n",
    "                no_of_iterations+=1\n",
    "            pred_err=self.cost_function(X_test,Y_test,W_new)\n",
    "            erroe_sum=erroe_sum + pred_err\n",
    "            print(erroe_sum) \n",
    "                \n",
    "        return erroe_sum/30\n",
    "        \n",
    "    \n",
    "    def k_fold_validation(self,X,Y,alpha_min,alpha_max,steps):\n",
    "        dataset=np.hstack((X,Y))\n",
    "        N=dataset.shape[0]        \n",
    "        fold=N//5    ## K is taken as 5\n",
    "        n=dataset.shape[1]        \n",
    "        k=5\n",
    "        k1=dataset[0:fold,:]\n",
    "        k2=dataset[fold:2*fold,:]\n",
    "        k3=dataset[2*fold:3*fold,:]\n",
    "        k4=dataset[3*fold:4*fold,:]\n",
    "        k5=dataset[4*fold:5*fold,:]\n",
    "        fold_list=[k1,k2,k3,k4,k5]\n",
    "        \n",
    "        \n",
    "        mean_square_error=[]\n",
    "        alpha_array=[]\n",
    "        \n",
    "        for alpha in np.linspace(alpha_min,alpha_max,steps):\n",
    "            erroe_sum=0.0\n",
    "            self.learning_rate=alpha\n",
    "            for i in range(k):\n",
    "                tmp=np.zeros((1,n))\n",
    "                data_test=fold_list[i]\n",
    "                X_test=data_test[:,0:n-1]\n",
    "                Y_test=data_test[:,n-1:n]\n",
    "                for j in range(k):\n",
    "                    if(j!=i):\n",
    "                        tmp=np.vstack((tmp,fold_list[j]))\n",
    "                data_train=tmp[1:,:]\n",
    "                X_train=data_train[:,0:n-1]\n",
    "                Y_train=data_train[:,n-1:n]\n",
    "                W_old=np.ones((n-1,1),dtype=float)\n",
    "                W_new=W_old\n",
    "                norm=100000*100000\n",
    "                no_of_iterations=0\n",
    "                while(norm > 0.001 and no_of_iterations < 1000):\n",
    "                    W_new=self.grad_descent(X_train,Y_train,W_old)\n",
    "                    norm=np.linalg.norm(W_new-W_old)\n",
    "                    W_old=W_new\n",
    "                    no_of_iterations+=1\n",
    "                \n",
    "                cost=self.cost_function(X_test,Y_test,W_new)               \n",
    "                erroe_sum=erroe_sum + cost\n",
    "\n",
    "            mean_square_error.append(erroe_sum/k)            \n",
    "            alpha_array.append(alpha)        \n",
    "        index=mean_square_error.index(min(mean_square_error))\n",
    "        return alpha_array[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f3b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Wine quality data set using linear regression (download data from UCI web\n",
    "#repository)\n",
    "#(a) Analyze the data with normalization and without normalization.\n",
    "#(b) Describe how you applied normalization techniques on training and testing data.\n",
    "#(c) Apply k fold cross validation and hold out method.\n",
    "#(d) Assess the performance of the model.\n",
    "#(e) Report search space & the values of the hyperparameters and the parameters of\n",
    "#the model.\n",
    "#(f) Apply batch as well as online optimization algorithms and compare their perfor-\n",
    "#mance using statistical measures. Compare the time taken by the two algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d5fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps followed\n",
    "   # 1. Normalize the data using min max normalizer\n",
    "   # 1. Find the weights and  cost function using direct method.\n",
    "   # 2. Find out the alpha value from hold out method and K fold iteration method\n",
    "   # 3. Use this alpha for gradient descent and find out whether the cost function coverges\n",
    "# Search space for alpha parameter is selected on trail and error basis as 0.01 to0.040\n",
    "# The best alpha was found out from hold out iteration as 0.035 and was selected as the hyper \n",
    "# parameter for gradient descent method\n",
    "# Weights obtained from both direct and gradient descent methods are saved in output folder\n",
    "# as .csv files\n",
    "# The final cost function and time taken for each methods are outputted from the cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42fd8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,n= 1599 12\n",
      "Shape of normalised: 1599 12\n",
      "time taken for direct method =  0.021725199999991673\n",
      "Parameters after direct method\n",
      "Cost for direct method =  [[0.00833534]]\n",
      "Total time taken for iterative method_batch_gradient (secs) =  4.0573179999999525\n",
      "Parameters after iterative batch gradient method\n",
      "Cost for iterative batch method  =  [[0.0083666]]\n",
      "Total time taken for iterative method_stochastic_gradient (secs): 97.46597409999998\n",
      "Parameters after iterative stochastic gradient method\n",
      "Cost for iterative online method  =  [[0.00843748]]\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "input_data5 = np.genfromtxt(\"data/winequality-red.csv\",delimiter=';',dtype=float)\n",
    "lrm = LinearRegressionModule(50,0.035)\n",
    "#print(input_data5)\n",
    "\n",
    "input_data5_processed = np.delete(input_data5, 0, axis=0)\n",
    "N=input_data5_processed.shape[0]\n",
    "\n",
    "data_normalized=lrm.minmax_normalization(input_data5_processed)\n",
    "#print(data_normalized)\n",
    "n=input_data5_processed.shape[1]\n",
    "ones_row=tuple(np.ones((N,1)))\n",
    "X=data_normalized[:, 0:n-1]  \n",
    "X=np.hstack((ones_row,X))\n",
    "Y=data_normalized[:,n-1:n]\n",
    "N=X.shape[0]\n",
    "n=X.shape[1]\n",
    "W_current=np.ones((n,1))\n",
    "print(\"Shape of normalised:\",N,n)\n",
    "W_new       =W_current\n",
    "start = timer()\n",
    "W_new       =lrm.direct_method_overdetermined(X,Y)\n",
    "stop = timer()\n",
    "total_time = stop-start\n",
    "print(\"time taken for direct method = \", total_time)\n",
    "print(\"Parameters after direct method\")\n",
    "cost = lrm.cost_function(X,Y,W_new)\n",
    "print(\"Cost for direct method = \" ,cost)\n",
    "lrm.write_weights_to_file(\"output/winequality-red-direct.csv\", W_new)\n",
    "\n",
    "W_current=np.ones((n,1))\n",
    "iteration_no=0\n",
    "norm=1000*10000\n",
    "W_new=W_current\n",
    "start = timer()\n",
    "while(norm > 0.00001 and iteration_no < 100000):\n",
    "    W_new=lrm.grad_descent(X,Y,W_current)    \n",
    "    norm=np.linalg.norm(W_new-W_current)   \n",
    "    W_current=W_new\n",
    "    cost=lrm.cost_function(X,Y,W_new)   \n",
    "    iteration_no+=1\n",
    "stop = timer()\n",
    "total_time = stop - start\n",
    "print (\"Total time taken for iterative method_batch_gradient (secs) = \" ,total_time)\n",
    "print(\"Parameters after iterative batch gradient method\")\n",
    "cost = lrm.cost_function(X,Y,W_new)\n",
    "print(\"Cost for iterative batch method  = \" ,cost)\n",
    "lrm.write_weights_to_file(\"output/winequality-red-batch-iteration.csv\",W_new)\n",
    "\n",
    "\n",
    "W_current=np.zeros((n,1))\n",
    "W_new=W_current\n",
    "norm=100000*100000\n",
    "start = timer()\n",
    "while(norm > 0.0001):\n",
    "    W_new=lrm.stochastic_descent(X,Y,W_current,0.10)\n",
    "    norm=np.linalg.norm(W_new-W_current)\n",
    "    W_current=W_new        \n",
    "    end = timer()    \n",
    "print(\"Total time taken for iterative method_stochastic_gradient (secs):\",end-start)\n",
    "print(\"Parameters after iterative stochastic gradient method\")\n",
    "\n",
    "\n",
    "cost = lrm.cost_function(X,Y,W_new)\n",
    "print(\"Cost for iterative online method  = \" ,cost)\n",
    "lrm.write_weights_to_file(\"output/winequality-red-online-iteration.csv\",W_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(input_data4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf1604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,n= 1599 12\n",
      "Shape of normalised: 1599 12\n",
      "Mean_square_Error_Minimum =  [[0.00579887]]\n",
      "alpha_minimum =  0.035\n",
      "Standard_Deviation_Minimum =  0.0020456459401918853\n"
     ]
    }
   ],
   "source": [
    "input_data5 = np.genfromtxt(\"data/winequality-red.csv\",delimiter=';',dtype=float)\n",
    "lrm = LinearRegressionModule(50,0.035)\n",
    "#print(input_data5)\n",
    "\n",
    "input_data5_processed = np.delete(input_data5, 0, axis=0)\n",
    "N=input_data5_processed.shape[0]\n",
    "\n",
    "data_normalized=lrm.minmax_normalization(input_data5_processed)\n",
    "#print(data_normalized)\n",
    "n=input_data5_processed.shape[1]\n",
    "ones_row=tuple(np.ones((N,1)))\n",
    "X=data_normalized[:, 0:n-1]  \n",
    "X=np.hstack((ones_row,X))\n",
    "Y=data_normalized[:,n-1:n]\n",
    "N=X.shape[0]\n",
    "n=X.shape[1]\n",
    "W_current=np.ones((n,1))\n",
    "print(\"Shape of normalised:\",N,n)\n",
    "\n",
    "a,b,c =lrm.hold_out_validation(X,Y,0.01,0.035,10)\n",
    "print(\"Mean_square_Error_Minimum = \",a)\n",
    "print(\"alpha_minimum = \",b)\n",
    "print(\"Standard_Deviation_Minimum = \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7089cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N,n= 1599 12\n",
      "Shape of normalised: 1599 12\n",
      "1599 13\n",
      " Alpha minimum for K-Fold for iteration no 1  is  0.035\n",
      "[[0.01249442]]\n",
      " Alpha minimum for K-Fold for iteration no 2  is  0.035\n",
      "[[0.02524502]]\n",
      " Alpha minimum for K-Fold for iteration no 3  is  0.035\n",
      "[[0.03892247]]\n",
      " Alpha minimum for K-Fold for iteration no 4  is  0.035\n",
      "[[0.05118431]]\n",
      " Alpha minimum for K-Fold for iteration no 5  is  0.035\n",
      "[[0.06495704]]\n",
      " Alpha minimum for K-Fold for iteration no 6  is  0.035\n",
      "[[0.07862224]]\n",
      " Alpha minimum for K-Fold for iteration no 7  is  0.035\n",
      "[[0.09110814]]\n",
      " Alpha minimum for K-Fold for iteration no 8  is  0.035\n",
      "[[0.10417246]]\n",
      " Alpha minimum for K-Fold for iteration no 9  is  0.035\n",
      "[[0.11685548]]\n",
      " Alpha minimum for K-Fold for iteration no 10  is  0.035\n",
      "[[0.12950395]]\n",
      " Alpha minimum for K-Fold for iteration no 11  is  0.035\n",
      "[[0.14325926]]\n",
      " Alpha minimum for K-Fold for iteration no 12  is  0.035\n",
      "[[0.15559621]]\n",
      " Alpha minimum for K-Fold for iteration no 13  is  0.035\n",
      "[[0.16896572]]\n",
      " Alpha minimum for K-Fold for iteration no 14  is  0.035\n",
      "[[0.18229593]]\n",
      " Alpha minimum for K-Fold for iteration no 15  is  0.035\n",
      "[[0.1953721]]\n",
      " Alpha minimum for K-Fold for iteration no 16  is  0.035\n",
      "[[0.20889368]]\n",
      " Alpha minimum for K-Fold for iteration no 17  is  0.035\n",
      "[[0.22229422]]\n",
      " Alpha minimum for K-Fold for iteration no 18  is  0.035\n",
      "[[0.23559149]]\n",
      " Alpha minimum for K-Fold for iteration no 19  is  0.035\n",
      "[[0.24739878]]\n",
      " Alpha minimum for K-Fold for iteration no 20  is  0.035\n",
      "[[0.25931853]]\n",
      " Alpha minimum for K-Fold for iteration no 21  is  0.035\n",
      "[[0.2715108]]\n",
      " Alpha minimum for K-Fold for iteration no 22  is  0.035\n",
      "[[0.28442123]]\n",
      " Alpha minimum for K-Fold for iteration no 23  is  0.035\n",
      "[[0.29693627]]\n",
      " Alpha minimum for K-Fold for iteration no 24  is  0.035\n",
      "[[0.30928834]]\n",
      " Alpha minimum for K-Fold for iteration no 25  is  0.035\n",
      "[[0.3215974]]\n",
      " Alpha minimum for K-Fold for iteration no 26  is  0.035\n",
      "[[0.3344396]]\n",
      " Alpha minimum for K-Fold for iteration no 27  is  0.035\n",
      "[[0.34660457]]\n",
      " Alpha minimum for K-Fold for iteration no 28  is  0.035\n",
      "[[0.35995023]]\n",
      " Alpha minimum for K-Fold for iteration no 29  is  0.035\n",
      "[[0.37391367]]\n",
      " Alpha minimum for K-Fold for iteration no 30  is  0.035\n",
      "[[0.38593189]]\n",
      "MSE: [[0.0128644]]\n"
     ]
    }
   ],
   "source": [
    "input_data5 = np.genfromtxt(\"data/winequality-red.csv\",delimiter=';',dtype=float)\n",
    "lrm = LinearRegressionModule(50,0.035)\n",
    "#print(input_data5)\n",
    "\n",
    "input_data5_processed = np.delete(input_data5, 0, axis=0)\n",
    "N=input_data5_processed.shape[0]\n",
    "\n",
    "data_normalized=lrm.minmax_normalization(input_data5_processed)\n",
    "#print(data_normalized)\n",
    "n=input_data5_processed.shape[1]\n",
    "ones_row=tuple(np.ones((N,1)))\n",
    "X=data_normalized[:, 0:n-1]  \n",
    "X=np.hstack((ones_row,X))\n",
    "Y=data_normalized[:,n-1:n]\n",
    "N=X.shape[0]\n",
    "n=X.shape[1]\n",
    "W_current=np.ones((n,1))\n",
    "print(\"Shape of normalised:\",N,n)\n",
    "W_new       =W_current\n",
    "validation_data=np.hstack((X,Y))\n",
    "mse=lrm.k_fold_cross_validation(validation_data,0.001,0.035,20)\n",
    "print(\"MSE:\",mse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbc2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
